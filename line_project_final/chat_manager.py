from llama_index.llms.ollama import Ollama
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.chat_engine.context import ContextChatEngine
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.prompts import PromptTemplate
from transformers import pipeline
import os


def manage_chat(user_id, user_name):
    input_dir_path = f'./chat_record/{user_name}'
    
    # è¨­å®šåµŒå…¥æ¨¡å‹å’Œ LLM
    llm = Ollama(model="gemma3:27b", request_timeout=120.0)

    embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-m3", trust_remote_code=True)

    # **è¼‰å…¥æ•¸æ“š**
    loader = SimpleDirectoryReader(input_dir=input_dir_path, required_exts=[".txt"], recursive=True)
    docs = loader.load_data()

    # **å‰µå»ºç´¢å¼•**
    index = VectorStoreIndex.from_documents(docs, embed_model=embed_model, show_progress=True)


    # **å‰µå»ºæª¢ç´¢å™¨**
    retriever = VectorIndexRetriever(index=index)
    retriever.retrieve("æœ€è¿‘çš„è«®è©¢ç´€éŒ„")


    # **å‰µå»ºè¨˜æ†¶é«”ç·©è¡å€**
    memory = ChatMemoryBuffer(token_limit=10000)

    # **è¨­å®š Prompt**
    prompt = """
    ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å¿ƒç†è«®å•†è¨˜éŒ„åˆ†æå¸«ï¼Œå°ˆé–€è² è²¬æ•´ç†å¿ƒç†è«®å•†å°è©±çš„é‡é»ï¼Œä¸¦è©•ä¼°è«®è©¢æ˜¯å¦æˆåŠŸã€‚ä½ çš„ä»»å‹™åŒ…æ‹¬ï¼š
    1. **æ‘˜è¦æœ¬æ¬¡è«®è©¢çš„æ ¸å¿ƒå…§å®¹**ï¼š
    - æ‚£è€…çš„ä¸»è¦å›°æ“¾æ˜¯ä»€éº¼ï¼Ÿ
    - æ‚£è€…åœ¨å°è©±ä¸­è¡¨é”äº†å“ªäº›æƒ…ç·’è®ŠåŒ–ï¼Ÿ
    - é€™æ¬¡è«®è©¢çš„é—œéµè½‰æŠ˜é»æ˜¯ä»€éº¼ï¼Ÿï¼ˆä¾‹å¦‚æ‚£è€…é–‹å§‹é¡˜æ„è¡¨é”æ„Ÿå—ã€æ¥å—å»ºè­°ï¼‰
    2. **åˆ†æè«®è©¢éç¨‹**ï¼š
    - æ©Ÿå™¨äººå¦‚ä½•å¼•å°æ‚£è€…è¡¨é”æƒ…ç·’ï¼Ÿ
    - æ˜¯å¦æä¾›äº†é©ç•¶çš„å»ºè­°ï¼Ÿ
    - æ‚£è€…æ˜¯å¦æ¥å—é€™äº›å»ºè­°ï¼Ÿ
    3. **åˆ¤æ–·è«®è©¢æ˜¯å¦æˆåŠŸ**ï¼š
    - æ‚£è€…æ˜¯å¦è¡¨é”æ„Ÿè¬æˆ–æƒ…ç·’æ˜é¡¯æ”¹å–„ï¼Ÿ
    - ä»–æ˜¯å¦è¡¨ç¤ºæ„Ÿè¦ºã€Œå¥½ä¸€äº›äº†ã€æˆ–ã€Œå•é¡Œå¾—åˆ°å¹«åŠ©ã€ï¼Ÿ
    - æ˜¯å¦ä»ç„¶å­˜åœ¨æœªè§£æ±ºçš„å•é¡Œï¼Ÿ
    - æ˜¯å¦éœ€è¦é€²ä¸€æ­¥è·Ÿé€²ï¼Ÿ

    è«‹ä»¥ç°¡å–®æ˜“æ‡‚çš„æ–¹å¼æ•´ç†é‡é»ï¼Œä¸è¦é€å­—æŠ„å¯«å°è©±ã€‚æœ€å¾Œè«‹çµ¦å‡ºå°é€™æ¬¡è«®è©¢çš„**ç¸½çµèˆ‡è©•ä¼°**ï¼Œä¸¦æ¨™ç¤ºã€Œè«®è©¢æˆåŠŸã€æˆ–ã€Œéœ€è¦é€²ä¸€æ­¥è«®è©¢ã€ã€‚

    **è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š**

    **ğŸ“Œ è«®è©¢æ‘˜è¦ï¼š**
    - **æ‚£è€…å§“å**ï¼š{user_name}
    - **ä¸»è¦å›°æ“¾**ï¼š{patient_issue}
    - **æƒ…ç·’è®ŠåŒ–**ï¼š
    - åˆå§‹ç‹€æ…‹ï¼š{initial_emotion}
    - éç¨‹ä¸­è®ŠåŒ–ï¼š{emotion_changes}
    - æœ€çµ‚ç‹€æ…‹ï¼š{final_emotion}
    - **é—œéµå°è©±è½‰æŠ˜é»**ï¼š
    - {turning_points}

    **ğŸ“Š åˆ†æèˆ‡è©•ä¼°ï¼š**
    - **æ©Ÿå™¨äººå¼•å°æ–¹å¼**ï¼š{guidance_analysis}
    - **æ‚£è€…å°å»ºè­°çš„åæ‡‰**ï¼š{response_to_advice}
    - **æ˜¯å¦éœ€è¦å¾ŒçºŒè«®è©¢**ï¼š{follow_up_needed}

    **âœ… è«®è©¢çµæœï¼š**
    {consultation_result}
    """

    # context_template = PromptTemplate(prompt)

    # **å‰µå»º ContextChatEngine**
    chat_engine = ContextChatEngine(
        retriever=retriever,
        llm=llm,
        memory=memory,
        prefix_messages=[],
        # context_template=context_template
    )

    path = f"./chat_analyzed/{user_name}"
    if not os.path.isdir(path):
        os.mkdir(path)

    # **é–‹å§‹è«®è©¢ç´€éŒ„åˆ†æ**
    response = chat_engine.chat(prompt)

    # **å„²å­˜æ•´ç†å¾Œçš„ç´€éŒ„**
    with open(f"./chat_analyzed/{user_name}/{user_name}çš„è«®è©¢æ•´ç†.txt", 'a') as f:
        f.write(str(response))

    print(f"ğŸ‰ ç”¨æˆ¶[ {user_name} ]å°è©±ç´€éŒ„å½™æ•´å®Œç•¢")
