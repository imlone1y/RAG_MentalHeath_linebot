import os

# --------------------- ä»¥ä¸‹ä¿ç•™ llamaindex.py å…§åŸæœ¬çš„ import -----------------------
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.chat_engine.context import ContextChatEngine
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.prompts import PromptTemplate
from prompt import assistant_prompt

# --------------------- ä»¥ä¸‹ç‚º rag_test.py æ‰€éœ€çš„ import -----------------------
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core import QueryBundle
from llama_index.core.schema import Node, NodeWithScore
from langchain.schema import Document as LangChainDocument
from langchain_community.retrievers import BM25Retriever
from typing import List

# --------------------- rag_test.py ä¸­çš„è‡ªå®šç¾© Hybrid Retriever ----------------------
class CustomHybridRetriever(BaseRetriever):
    def __init__(self, vector_retriever, keyword_retriever, mode="OR"):
        self.vector_retriever = vector_retriever
        self.keyword_retriever = keyword_retriever
        self.mode = mode  # æ”¯æ´ "OR" æˆ– "AND"

    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
        vector_nodes = self.vector_retriever.retrieve(query_bundle)
        keyword_docs = self.keyword_retriever.invoke(query_bundle.query_str)

        vector_content_map = {n.node.get_content(): n for n in vector_nodes}
        keyword_content_map = {d.page_content: d for d in keyword_docs}

        if self.mode == "AND":
            shared_keys = set(vector_content_map.keys()).intersection(set(keyword_content_map.keys()))
        else:  # OR æ¨¡å¼
            shared_keys = set(vector_content_map.keys()).union(set(keyword_content_map.keys()))

        # å°‡ keyword_docs è£œæˆ NodeWithScore
        converted_keyword_nodes = []
        for content in keyword_content_map:
            if content not in vector_content_map:  # é¿å…é‡è¤‡åŠ å…¥
                fake_node = NodeWithScore(node=Node(text=content, metadata={}), score=0.0)
                converted_keyword_nodes.append(fake_node)

        return list(vector_content_map.values()) + converted_keyword_nodes


# --------------------- ä»¥ä¸‹ä¿ç•™ llamaindex.py åŸæœ¬æ ¸å¿ƒé‚è¼¯ï¼šç®¡ç†å¤šä½¿ç”¨è€… + å°è©± ----------------------
user_chat_engines = {}

def assistant_create(user_id, user_name):
    """ç‚ºç‰¹å®šä½¿ç”¨è€…å‰µå»º ChatEngineï¼Œä¸¦æ›¿æ› RAG ç‚º Hybrid Retriever"""
    global user_chat_engines

    # åŸæœ¬çš„éƒ¨åˆ†
    input_dir_path = './files'
    llm = Ollama(
        model="gemma3:27b",
        request_timeout=120.0
    )
    embed_model = HuggingFaceEmbedding(
        model_name="BAAI/bge-m3",
        trust_remote_code=True
    )

    # ------------------- ä»¥ä¸‹æ›¿æ›æˆ rag_test çš„åšæ³•ï¼šè¼‰å…¥è³‡æ–™ + å»º Hybrid Retriever -------------------
    documents = SimpleDirectoryReader(
        input_dir=input_dir_path, 
        required_exts=[".pdf"],  # ä¾å¯¦éš›éœ€æ±‚èª¿æ•´
        recursive=True
    ).load_data()

    # åˆ‡åˆ†æ®µè½
    parser = SentenceSplitter(chunk_size=256, chunk_overlap=64)
    nodes = parser.get_nodes_from_documents(documents)

    # å»ºç«‹å‘é‡ç´¢å¼•
    index = VectorStoreIndex(nodes, embed_model=embed_model)
    vector_retriever = VectorIndexRetriever(index=index, similarity_top_k=5)

    # å»ºç«‹ BM25 Retriever
    langchain_documents = [
        LangChainDocument(page_content=doc.get_content(), metadata=doc.metadata)
        for doc in documents
    ]
    keyword_retriever = BM25Retriever.from_documents(langchain_documents)

    # Hybrid Retriever
    retriever = CustomHybridRetriever(
        vector_retriever=vector_retriever,
        keyword_retriever=keyword_retriever,
        mode="OR"
    )

    # ------------------- å…¶é¤˜(è¨˜æ†¶é«”ã€promptã€ContextChatEngine)ä¿æŒä¸å‹• -------------------
    memory = ChatMemoryBuffer(token_limit=10000)

    prompt, profile = assistant_prompt(user_name)
    context_template = PromptTemplate(prompt)

    # å»ºç«‹æ‚£è€…å°è©±ç´€éŒ„è³‡æ–™å¤¾
    path = f"./chat_record/{user_name}"
    if not os.path.isdir(path):
        os.mkdir(path)

    # å¯«å…¥æ‚£è€…åŸºæœ¬è³‡æ–™
    with open(f"./chat_record/{user_name}/{user_id}.txt", 'w', encoding='utf-8') as f:
        f.write("---------------------------------------------\n")
        f.write(f"{profile}\n")
        f.write("---------------------------------------------\n")

    # ä½¿ç”¨ ContextChatEngineï¼Œä½† retriever æ›æˆè‡ªè¨‚ Hybrid
    user_chat_engines[user_id] = ContextChatEngine(
        retriever=retriever,
        llm=llm,
        memory=memory,
        prefix_messages=[],
        context_template=context_template
    )

    print(f"âœ… ChatEngine åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨è€… {user_name} ({user_id})")

def assistant_reply(user_id, user_name, message):
    """æ ¹æ“šä½¿ç”¨è€… ID å›æ‡‰è¨Šæ¯ï¼Œä¸¦å°å‡º RAG æœå°‹ä¾æ“š"""
    global user_chat_engines

    # é€²è¡Œå°è©±
    response = user_chat_engines[user_id].chat(str(message))

    # å°‡å›æ‡‰è¨˜éŒ„åˆ°æª”æ¡ˆ
    with open(f"./chat_record/{user_name}/{user_id}.txt", 'a', encoding='utf-8') as f:
        f.write(f"client: {message}\n")
        f.write("---------------------------------------------\n")
        f.write(f"mental health: {response}\n")
        f.write("---------------------------------------------\n")

    # ------------------- ä»¥ä¸‹ç‚ºå°å‡ºæœå°‹ä¾æ“šçš„åŠŸèƒ½ï¼ˆæ¨¡ä»¿ rag_test.pyï¼‰ -------------------
    print("\n=== ğŸ” å›ç­” ===")
    print(str(response))

    # æ³¨æ„ï¼šä»¥ä¸‹ç¤ºç¯„å¦‚æœ response æ²’æœ‰ source_nodes å±¬æ€§ï¼Œéœ€è¦å…ˆæª¢æŸ¥
    print("\n=== ğŸ“„ è³‡æ–™ä¾†æº ===")
    if hasattr(response, "source_nodes") and response.source_nodes:
        for i, node in enumerate(response.source_nodes):
            score = getattr(node, "score", "N/A")
            # metadata å¯èƒ½å­˜æœ‰ 'file_path' æˆ–å…¶ä»–é—œéµè³‡è¨Šï¼Œä¾å¯¦éš›æƒ…æ³èª¿æ•´
            file_path = node.node.metadata.get('file_path') if hasattr(node, 'node') else ""
            content = node.node.get_content()[:200] if hasattr(node, 'node') else "N/A"

            print(f"[ä¾†æº {i+1}] (score: {score}) ä¾†è‡ªæª”æ¡ˆ: {file_path}")
            print(content)
            print("-" * 40)
    else:
        print("æ²’æœ‰ä¾†æºè³‡æ–™æˆ– response ä¸åŒ…å« source_nodes")

    return str(response)
